\section{Methodology}
\label{ch:methodology}

\subsection{Research Approach and Design Philosophy}
This thesis adopts a \textbf{design science research methodology} \cite{hevner2004design} to develop and evaluate a closed-loop neuromorphic benchmark for electric motor control. The research artifact consists of three interconnected components: (1) a standardized simulation environment for permanent magnet synchronous motor (PMSM) control, (2) a spiking neural network controller synthesized via imitation learning, and (3) a comprehensive metrics framework that extends the NeuroBench standard \cite{yik_neurobench_2025} to continuous control tasks.

The methodological approach follows an iterative development cycle comprising \textbf{design}, \textbf{implementation}, \textbf{demonstration}, and \textbf{evaluation} phases. This structure enables systematic validation of the benchmark framework against established control engineering baselines while ensuring compatibility with neuromorphic computing standards.

\subsection{Simulation Environment}

\subsubsection{Simulation Framework}
The simulation environment is implemented using the \textbf{gym-electric-motor (GEM)} framework \cite{balakrishna_gym-electric-motor_2021}. GEM is selected over alternative simulators (MATLAB/Simulink) for its native Python integration and Gymnasium compatibility, which facilitates the interface with PyTorch-based SNN frameworks.

\subsubsection{Control Loop Configuration}
The benchmark operates at a control frequency of \textbf{10~kHz} ($T_s = 100~\mu s$). The control hierarchy follows the standard Field-Oriented Control (FOC) structure. The SNN replaces the inner current control loop, receiving current errors as inputs and producing voltage commands $(u_d^*, u_q^*)$ as outputs.

\subsection{Neuromorphic Controller Architecture}
To ensure robust control performance, the SNN is synthesized using a three-stage \textbf{Imitation Learning (Behavioral Cloning)} workflow.

\subsubsection{Stage 1: Data Acquisition (The Teacher)}
A conventional PI controller, tuned via the Symmetrical Optimum method \cite{leonhard_control_2001}, acts as the "Teacher." The simulation is executed across the full operating envelope to generate a labeled dataset $\mathcal{D} = \{(x_t, y_t)\}_{t=0}^T$:
\begin{itemize}
    \item \textbf{Input Data ($x_t$):} Sensor measurements fed into the network, specifically the error signals $(e_d, e_q)$ and current state $(i_d, i_q)$.
    \item \textbf{Output Data ($y_t$):} The target voltage commands $(u_d^*, u_q^*)$ generated by the PI controller.
\end{itemize}

\subsubsection{Stage 2: ANN Replication}
Before introducing spiking dynamics, a standard Artificial Neural Network (ANN) is trained to replicate the control law of the PI teacher. This step utilizes standard deep learning frameworks (e.g., PyTorch or Edge Impulse) to minimize the Mean Squared Error (MSE) between the network prediction and the Teacher's output. The ANN utilizes \textbf{Rectified Linear Units (ReLU)} as activation functions to learn the non-linear mapping of the control surface.

\subsubsection{Stage 3: SNN Conversion (ReLU $\to$ LIF)}
To enable neuromorphic execution, the trained ANN is converted into a Spiking Neural Network (SNN). This involves replacing the continuous ReLU activation functions with \textbf{Leaky Integrate-and-Fire (LIF)} neurons.
\begin{equation}
    \tau_m \cdot \frac{dV}{dt} = -(V - V_{rest}) + R_m \cdot I_{syn}
    \label{eq:lif_dynamics}
\end{equation}
The continuous output of the ReLU units is mapped to the firing rate of the LIF neurons. This conversion strategy allows the SNN to inherit the accuracy of the pre-trained ANN while leveraging the energy efficiency of event-driven sparsity.

\subsection{Benchmark Framework Integration}

\subsubsection{NeuroBench Extension}
The benchmark framework builds upon \textbf{NeuroBench} \cite{yik_neurobench_2025}. This thesis extends the framework with motor control-specific components:
\begin{description}
    \item[Environment Wrapper:] Adapts the GEM PMSM simulation to NeuroBench's Gymnasium protocol.
    \item[State Management:] Explicitly handles the persistent neuron states required for closed-loop control, unlike feedforward classification tasks.
\end{description}

\subsubsection{Benchmark Scenarios}
The benchmark defines systematic evaluation scenarios:
\begin{itemize}
    \item \textbf{Scenario 1 - Step Response:} Dynamic response to step changes in current references to evaluate rise time and overshoot.
    \item \textbf{Scenario 2 - Operating Point Sweep:} Evaluation across the entire torque-speed envelope (MTPA and Field Weakening).
    \item \textbf{Scenario 3 - Disturbance Rejection:} Response to load torque steps to assess robustness.
\end{itemize}

\subsection{Evaluation Metrics (To be discussed with bene)}

\subsubsection{Control Performance Metrics}
\textbf{Accuracy Metrics:}
\begin{itemize}
    \item \textbf{ITAE:} $\int_0^T t \cdot |e(t)| \, dt$ penalizes persistent steady-state errors.
    \item \textbf{MAE:} Mean Absolute Error for interpretable average deviation.
\end{itemize}

\textbf{Dynamic Metrics:}
\begin{itemize}
    \item \textbf{Rise Time ($t_r$):} Duration from 10\% to 90\% of final value.
    \item \textbf{Overshoot ($M_p$):} Peak deviation above reference.
\end{itemize}

\subsubsection{Neuromorphic Efficiency Metrics}
\begin{itemize}
    \item \textbf{Synaptic Operations (SyOps):} The total count of spike-triggered accumulations per inference step.
    \item \textbf{Activation Sparsity:} The fraction of neurons that remain inactive during inference.
    \item \textbf{Estimated Energy:} Calculated using hardware-specific coefficients (e.g., 23 pJ/Op for Loihi 2).
\end{itemize}

